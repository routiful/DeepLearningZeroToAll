{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2. Linear regression.ipynb","provenance":[{"file_id":"1bRj9uh6feo_pTxxZzbYFmEygtTVp8q-P","timestamp":1586932104263}],"collapsed_sections":[],"authorship_tag":"ABX9TyMDZxuJPWybXTEUJ7Squk0g"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"e2wp-_zv5eMz","colab_type":"text"},"source":["**1. Initialization**"]},{"cell_type":"code","metadata":{"id":"uCEMbx295Nuh","colab_type":"code","colab":{}},"source":["from __future__ import absolute_import, division, print_function, unicode_literals\n","!pip install -q tensorflow==2.0.0-beta1\n","\n","import tensorflow as tf"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VPDJxanUF7lJ","colab_type":"text"},"source":["**2. Linear regression**"]},{"cell_type":"code","metadata":{"id":"QjgBg0DrF6PM","colab_type":"code","outputId":"8193806f-d067-4573-d1f9-ea01629623e4","executionInfo":{"status":"ok","timestamp":1586932536816,"user_tz":-540,"elapsed":1835,"user":{"displayName":"끼리","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjaKJCogERkbCc2BdkT2OfKuWi47UBnkv7VOY9tTQ=s64","userId":"09706101366547236509"}},"colab":{"base_uri":"https://localhost:8080/","height":462}},"source":["x_train = tf.Variable([1.0, 2.0, 3.0], tf.float32)\n","y_train = tf.Variable([1.0, 2.0, 3.0], tf.float32)\n","\n","W = tf.Variable(tf.random.uniform(shape=[], minval=0, maxval=1, dtype=tf.float32, seed=3))\n","b = tf.Variable(tf.random.uniform(shape=[], minval=0, maxval=1, dtype=tf.float32, seed=3))\n","\n","print(\"Initialization: \")\n","print(\"\\tW: \", W.numpy())\n","print(\"\\tb: \", b.numpy())\n","\n","@tf.function\n","def hypothesis(x):\n","  return x * W + b\n","\n","print(\"\\tH(x): \", hypothesis(x_train).numpy())\n","\n","@tf.function\n","def cost(x, y):\n","    return tf.reduce_mean(tf.square(hypothesis(x) - y))\n","\n","print(\"\\tcost(W, b): \", cost(x_train, y_train).numpy())\n","\n","optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n","loss = lambda: cost(x_train, y_train) \n","\n","for step in range(1000):\n","    optimizer.minimize(loss, var_list=[W, b])\n","    if step % 100 == 0: \n","        print(\"\\tstep[{}] cost(W, b): {}\".format(step, cost(x_train, y_train).numpy()))\n","\n","print(\"Optimized function: \")\n","print(\"\\tH(x): \", hypothesis(x_train).numpy())\n","print(\"\\tcost(W, b): \", cost(x_train, y_train).numpy())\n","\n","print(\"Optimized var: \")\n","print(\"\\tW: \", W.numpy())\n","print(\"\\tb: \", b.numpy())"],"execution_count":13,"outputs":[{"output_type":"stream","text":["Initialization: \n","\tW:  0.015974998\n","\tb:  0.38444567\n","WARNING:tensorflow:Entity <function hypothesis at 0x7f0979652f28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function hypothesis at 0x7f0979652f28>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <function hypothesis at 0x7f0979652f28> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function hypothesis at 0x7f0979652f28>: AssertionError: Bad argument number for Name: 3, expecting 4\n","\tH(x):  [0.40042067 0.41639566 0.43237066]\n","WARNING:tensorflow:Entity <function cost at 0x7f0979652e18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cost at 0x7f0979652e18>: AssertionError: Bad argument number for Name: 3, expecting 4\n","WARNING: Entity <function cost at 0x7f0979652e18> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <function cost at 0x7f0979652e18>: AssertionError: Bad argument number for Name: 3, expecting 4\n","\tcost(W, b):  3.1533394\n","\tstep[0] cost(W, b): 2.5063209533691406\n","\tstep[100] cost(W, b): 0.04137174040079117\n","\tstep[200] cost(W, b): 0.025565216317772865\n","\tstep[300] cost(W, b): 0.015797751024365425\n","\tstep[400] cost(W, b): 0.009762045927345753\n","\tstep[500] cost(W, b): 0.006032351404428482\n","\tstep[600] cost(W, b): 0.003727635135874152\n","\tstep[700] cost(W, b): 0.0023034531623125076\n","\tstep[800] cost(W, b): 0.001423394656740129\n","\tstep[900] cost(W, b): 0.0008795738103799522\n","Optimized function: \n","\tH(x):  [1.0345588 2.0074162 2.9802737]\n","\tcost(W, b):  0.0005461452\n","Optimized var: \n","\tW:  0.97285753\n","\tb:  0.06170118\n"],"name":"stdout"}]}]}